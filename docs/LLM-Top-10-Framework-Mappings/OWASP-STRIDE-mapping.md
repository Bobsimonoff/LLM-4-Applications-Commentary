By Bob Simonoff

- LinkedIn at https://www.linkedin.com/in/bob-simonoff

- medium.com at https://medium.com/@bob.simonoff

- github at https://github.com/Bobsimonoff/LLM-4-Applications-Commentary/tree/main


# LLM01: Prompt Injection

### Summary

Crafted prompts can manipulate LLMs to cause unauthorized access, data breaches, and compromised decision-making.


### STRIDE Analysis (generated by claude.ai)

**Spoofing**

- Attackers can spoof the origin of injected prompts to disguise their source. 
- Injected prompts could also spoof user identity or authentication credentials to access restricted data and functions.

**Tampering** 

- Malicious prompts can tamper with model behaviors, predictions, system interactions, and data.
- Carefully crafted prompts precisely control model actions through malicious injected instructions.

**Repudiation**

- Lack of logging around injected prompts can complicate attack attribution.
- Prompt injection attacks could also tamper with or disable logging to undermine attribution.

**Information Disclosure**

- Injected prompts can trick models into revealing sensitive information about users, systems, or training data.
- Indirect injection through web inputs could extract sensitive data via malicious text processing.

**Denial of Service**

- Specially crafted prompts could trigger crashes, resource exhaustion, or make models unusable.
- Web inputs with recursive prompts could cause infinite inference loops.

**Elevation of Privilege**

- Injected prompts could escalate privileges or disable access controls on backend systems.  
- Compromised credentials enable bypassing access controls when injecting prompts.


# LLM02: Insecure Output Handling

### Summary 

Failing to validate, sanitize and filter LLM outputs enables attackers to indirectly access systems or trigger exploits via crafted prompts.


### STRIDE Analysis (generated by claude.ai)

**Spoofing**

- Attackers could spoof downstream systems by manipulating LLM outputs to impersonate expected data structures and bypass security checks.

- Malicious outputs could also spoof user identity or authentication credentials to access restricted data and functions.

**Tampering**

- Injected payloads via insecure outputs could tamper with downstream systems' control flows, causing unauthorized state changes.

- Carefully engineered outputs precisely control downstream behaviors through malicious instructions.

**Repudiation** 

- Lack of logging around LLM outputs complicates attack attribution and forensic investigations.

- Tampering with or disabling logs could also undermine attack attribution.

**Information Disclosure**

- Insecure handling of outputs could leak sensitive data like credentials, personal information, or intellectual property.

- Error messages and logs may reveal information if outputs are not properly sanitized.

**Denial of Service**

- Memory corruption, resource exhaustion, crashes, and instability could occur in downstream systems when processing malicious outputs.

- Infinite loops or complexity attacks could also be triggered by harmful outputs.

**Elevation of Privilege**

- Code execution via insecure outputs could allow escalating from low to high privilege levels on downstream systems.

- Horizontal privilege escalation is also possible through compromised credentials or bypassed access controls.


# LLM03: Training Data Poisoning

### Summary

Tampered training data can impair LLMs, leading to compromised security, accuracy, or ethical behavior.


### STRIDE Analysis (generated by claude.ai)

**Spoofing**

- Attackers can spoof the source of poisoned data to disguise its malicious origins.
- Manipulated training data could also spoof legitimate source domains, authors, timestamps, etc. 

**Tampering**

- Poisoning directly tampers with and manipulates model training data.
- Crafted malicious data precisely controls how models behave by embedding specific vulnerabilities.

**Repudiation** 

- Lack of data provenance around poisoned sources can complicate attack attribution.
- Poisoning attacks could also disable or tamper with data logging to undermine attribution.

**Information Disclosure**

- Models trained on poisoned data may reveal sensitive information about training data, model parameters, or backend systems.
- Poisoned web scraped data could expose sensitive user info during training.

**Denial of Service**

- Maliciously corrupted training data could trigger crashes, resource exhaustion, or model degradation.
- Targeted data poisoning could severely impair model accuracy or usability.

**Elevation of Privilege**

- Poisoned data may enable privilege escalation or disable access controls on backend systems.
- Compromised data source credentials enable greater access when poisoning data.


# LLM04: Denial of Service

### Summary
Overloading LLMs with resource-intensive operations can cause service disruptions, degraded performance, and increased costs.


### STRIDE Analysis (generated by claude.ai)

**Spoofing**

- Attackers can spoof the origin of requests to disguise their source and make attribution more difficult.

**Tampering**

- Malicious requests can tamper with model behaviors by overloading systems and disrupting availability.

**Repudiation** 

- Lack of logging around malicious requests complicates attack attribution and allows repudiation.
- DoS attacks could also tamper with or disable logging to undermine attribution.
- Flooding attacks against a language model can overwhelm any logging or monitoring capabilities on that system, making it difficult to record and analyze the high volumes of malicious traffic. This complicates root cause analysis and attribution after the attack.

**Information Disclosure**

- Flooding attacks do not directly cause information disclosure but service disruption enables easier access to compromised systems.

**Denial of Service**

- Specially crafted inputs trigger resource exhaustion, system crashes, and service disruption.
- Flooding systems with requests disrupts availability and prevents legitimate use.

**Elevation of Privilege**

- While DoS does not directly enable privilege escalation, service disruption provides opportunities for unauthorized access.
- Compromised credentials allow attackers to more easily trigger DoS conditions.  



# LLM05: Supply Chain Vulnerabilities

### Summary 

Depending on compromised third-party components can undermine system integrity, causing data breaches and failures.


### STRIDE Analysis (generated by claude.ai)

**Spoofing**

- Attackers can spoof compromised suppliers to hide their origin and insert vulnerabilities.
- Vulnerable third-party components enable spoofing other systems after infiltration.

**Tampering** 

- Compromised dependencies allow tampering with system integrity via malicious code insertion.
- Attackers can tamper with data at rest via compromised supply chain components. 

**Repudiation**

- Lack of integrity checks on third-party code undermines attack attribution.
- Compromised suppliers confound audit trails masking the source of attacks.

**Information Disclosure**

- Third-party components with data access enable unauthorized exposure of sensitive assets.
- Compromised suppliers can exfiltrate confidential data and intellectual property.

**Denial of Service**

- Vulnerable dependencies open avenues for crashing critical systems.
- Manipulation of third-party data and models can degrade system functionality.

**Elevation of Privilege** 

- Exploits in third-party code can be used to escalate privileges in downstream systems.
- Access permissions granted to suppliers for integration enable privilege escalation.


# LLM06: Sensitive Information Disclosure

### Summary

Insufficient safeguards risk exposing sensitive information through LLM outputs, causing legal issues or competitive harm.


### STRIDE Analysis (generated by claude.ai)


**Spoofing**

- Attackers can spoof user identities with stolen credentials to access sensitive data.
- Forged input data could trick models into revealing confidential information. 

**Tampering**

- Adversaries can tamper with inputs to manipulate model behavior for sensitive data exposure.
- Training data poisoning modifies model behavior to inadvertently leak sensitive information.

**Repudiation** 

- Lack of logging around access and data flows complicates attack attribution.
- Disablement of monitoring systems also undermines attack attribution.

**Information Disclosure**

- Improper data handling exposes sensitive information through models. 
- Unauthorized data access grants visibility into confidential artifacts and training data.

**Denial of Service**

- Unexpected crashes from adversarial inputs disrupt model availability.
- Resource exhaustion attacks on backend systems create denial of service effects.

**Elevation of Privilege**

- Compromised credentials enable escalated data access privileges.
- Exploiting vulnerabilities in access controls allows unauthorized data access.
- Circumventing authentication opens access to restricted data.


# LLM07: Insecure Plugin Design 

### Summary
LLM plugins processing untrusted inputs without validation can enable severe exploits like remote code execution.


### STRIDE Analysis (generated by claude.ai)

Insecure plugin design can impact multiple components of the STRIDE threat model:

**Spoofing**

- Attackers can spoof user identities through unauthorized access enabled by inadequate plugin authentication.
- Malicious plugins could also spoof other trusted system components to extract data.

**Tampering**

- Adversaries can tamper with data, predictions, and model behaviors by injecting malicious plugin inputs. 
- Carefully crafted plugin inputs precisely control model actions through malicious instructions.

**Repudiation** 

- Lack of logging around plugin inputs can complicate attack attribution.
- Plugin exploits could also tamper with or disable logging to hide their actions.

**Information Disclosure**

- Malicious plugins can trick models into revealing sensitive user, system, or training data.
- Indirect injection through web inputs could enable plugins to extract sensitive data via malicious text processing.

**Denial of Service**

- Specially crafted plugin inputs could trigger crashes, resource exhaustion, or make models unusable.
- Recursive plugin requests could cause infinite inference loops.

**Elevation of Privilege**

- Injected plugin inputs could escalate privileges or disable backend access controls.
- Compromised credentials enable bypassing access controls when invoking plugins.


# LLM08: Excessive Agency

### Summary

Excessive LLM permissions or autonomy enables unintended harmful actions based on faulty LLM outputs.


### STRIDE Analysis (generated by claude.ai)

**Spoofing**

- Adversaries can spoof user identities through valid credentials to bypass authorization controls when manipulating model behaviors.

- Excessive privileges granted to plugins and components can be abused to spoof trusted identities.

**Tampering**

- Unchecked model behaviors mean malicious inputs can precisely control and tamper with model actions and outputs.

- Excessive permissions enable tampering with downstream data and systems.

**Repudiation** 

- Lack of permissions auditing around manipulated behaviors can complicate attack attribution.

- Tampering with logs can undermine forensic analysis of exploited autonomous behaviors.


**Information Disclosure**

- Uncontrolled model outputs increase the risk of exposing sensitive information.

- Access to backend systems means private data can be extracted through manipulated behaviors.


**Denial of Service**

- Unchecked recursive model invocations could trigger resource exhaustion.

- Malicious adversarial samples could crash models or render them unusable.


**Elevation of Privilege**

- Excessive permissions granted to plugins can enable privilege escalation on backend systems.

- Valid accounts and poor access controls enable bypassing privilege restrictions.


# LLM09: Overreliance

### Summary

Blindly trusting LLM outputs can lead to issues like misinformation, legal problems, and reputational damage without verification.


### STRIDE Analysis (generated by claude.ai)

**Spoofing**

- LLMs can convincingly spoof accuracy and authority when generating erroneous or malicious outputs.
- Adversaries can spoof trusted sources and identities when manipulating LLM-generated content.

**Tampering**

- Blind trust enables adversaries to tamper with decision-making, system behaviors, and data flows.
- LLMs may unknowingly propagate altered information when overrelied upon.

**Repudiation** 

- Lack of validation around LLM outputs can complicate attack attribution.
- Overreliance may allow covering up tampering by accepting altered data or behaviors.

**Information Disclosure**

- Unverified LLM outputs could reveal sensitive information to unauthorized parties.
- Confidential data may be accidentally exposed when overrelying on LLM suggestions.

**Elevation of Privilege** 

- Adversaries can potentially escalate privileges by exploiting excessive trust in LLM-guided actions.
- LLM outputs could disable access controls if improperly implemented without oversight.



# LLM10: Model Theft

### Summary
LLM theft can lead to financial losses, competitive disadvantage, and unauthorized data access.


### STRIDE Analysis (generated by claude.ai)

**Spoofing**

- Adversaries can spoof or impersonate authorized users and system components to bypass authentication and access controls.

**Tampering**

- Stolen models may be tampered with through adversarial techniques like poisoning, parameter modification, or backdoors.

**Repudiation**

- Lack of access controls and monitoring around models enables denial of theft attribution. 

**Information Disclosure**

- Theft inherently discloses proprietary model IP and potentially sensitive training data.

**Denial of Service**

- Organizations lose exclusive availability and control over access to stolen proprietary models.

**Elevation of Privilege**

- Exploitation enables adversaries to gain privileged access to restricted models.







