By Bob Simonoff

- LinkedIn at https://www.linkedin.com/in/bob-simonoff

- medium/com at https://medium.com/@bob.simonoff

- github at https://github.com/Bobsimonoff/LLM-4-Applications-Commentary/tree/main


# LLM09: Overreliance

### Summary

Failing to critically assess LLM outputs can lead to compromised decision making, security vulnerabilities, and legal liabilities.

### Description

Overreliance can occur when an Large Language Model produces erroneous information and provides it in an authoritative manner. While LLMs can produce creative and informative content, they can also generate content that is factually incorrect, inappropriate or unsafe. This is referred to as hallucination or confabulation. When people or systems trust this information without oversight or confirmation it can result in a security breach, misinformation, miscommunication, legal issues, and reputational damage.

### Common Examples of Risk

1. LLM gives inaccurate information confidently, misleading users. 

2. LLM suggests insecure code, introducing vulnerabilities.


### Prevention and Mitigation Strategies

1. Continuously monitor and manually review LLM outputs.

2. Validate outputs by comparing to trusted external sources.

3. Use automated validations against known facts.

4. Fine-tune models to improve quality. 

5. Break complex tasks into smaller pieces across multiple agents.

6. Clearly communicate risks and limitations of LLMs.

7. Design interfaces that promote safe and responsible LLM use.

8. Establish secure coding practices when using LLM-generated code.

9. Implement human-in-the-loop approval for high risk or sensitive actions. 

10. Employ multiple levels of verification, not blindly trusting LLM outputs.

**Example High-Risk Scenarios:**

1. News org relies on LLM, spreading misinformation.

2. LLM plagiarizes, hurting trust.

3. Over-reliance on LLM coding suggestions introduces vulnerabilities. 

4. Developer adds malicious package, not verifying LLM suggestion.


### Common Weakness Enumeration (CWE)


- [CWE-829](https://cwe.mitre.org/data/definitions/829.html): Inclusion of Functionality from Untrusted Control Sphere

  Description: This weakness relates to the inclusion of functionality from an untrusted control sphere, which can lead to security issues.

  Justification: CWE-829 is applicable because relying on LLM outputs without validating the functionality can risk including untrusted code or logic, potentially leading to security vulnerabilities.


### MITRE ATT&CK® Techniques

N.A.


### MITRE ATLAS™ Techniques

- AML.T0011: User Execution  

  Description: Adversaries may manipulate users into executing unsafe actions or accessing malicious content based on incorrect information generated by an LLM that is over-relied upon. If users trust LLM outputs without proper validation, they may unknowingly execute unsafe code or actions that have been socially engineered by the adversary to take advantage of that blind trust.

  Justification: User execution of unsafe actions guided by over-reliance on incorrect LLM outputs enables adversaries to more readily manipulate user behavior through social engineering tactics designed to exploit the excessive trust placed in the LLM.

- AML.T0045: ML Intellectual Property Theft

  Description: Excessive trust in LLM outputs enables adversaries to more readily manipulate users into improperly disclosing or mishandling intellectual property and other confidential data assets by providing manipulated information that is incorrectly treated as authoritative without proper verification.

  Justification: Over-reliance on LLM outputs allows adversaries to more easily exploit users' excessive trust to steal IP and data through user actions guided by misleading LLM information.

- AML.T0046: Spamming ML System with Chaff Data

  Description: By overwhelming an LLM with excessive useless input designed to increase inaccurate outputs, an adversary can degrade the model's reliability over time, increasing the occurrence of incorrect outputs that users may over-rely on without proper verification.

  Justification: Spamming the LLM with useless chaff inputs manipulates the model to produce more inaccurate outputs, which users may blindly trust due to over-reliance, enabling adversary exploitation through the increased unreliable outputs.


### MITRE ATT&CK® Mitigations



### MITRE ATLAS™ Mitigations

- AML.M0018: User Training
  Description: Educate ML model developers on secure coding practices and ML vulnerabilities.
