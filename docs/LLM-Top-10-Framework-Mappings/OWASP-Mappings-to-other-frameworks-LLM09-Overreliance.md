By Bob Simonoff

- LinkedIn at https://www.linkedin.com/in/bob-simonoff

- medium.com at https://medium.com/@bob.simonoff

- github at https://github.com/Bobsimonoff/LLM-4-Applications-Commentary/tree/main


# LLM09: Overreliance

### Summary

Blindly trusting LLM outputs can lead to issues like misinformation, legal problems, and reputational damage without verification.

### Description

Overreliance can occur when an Large Language Model produces inaccurate, inappropriate, or unsafe information and provides it in an authoritative manner. When people or systems trust this information without oversight or confirmation it can result in a security breach, misinformation, miscommunication, legal issues, and reputational damage.

The core risk is blind trust in LLM output without verification, which enables harms from erroneous information. Awareness, validation and oversight are key to prevent overreliance.

Not to be confused with:
- LLM02: Insecure Output Handling - Failing to validate LLM generated outputs can create an attack vector downstream systems, whereas overreliance is about blindly trusting even validated outputs.


### Common Examples of Risk

1. LLM gives inaccurate information confidently, misleading users. 

2. LLM suggests insecure code, introducing vulnerabilities.


### Prevention and Mitigation Strategies

1. Continuously monitor and manually review LLM outputs.

2. Validate outputs by comparing to trusted external sources.

3. Use automated validations against known facts.

4. Fine-tune models to improve quality. 

5. Break complex tasks into smaller pieces across multiple agents.

6. Clearly communicate risks and limitations of LLMs.

7. Design interfaces that promote safe and responsible LLM use.

8. Establish secure coding practices when using LLM-generated code.

9. Implement human-in-the-loop approval for high risk or sensitive actions. 

10. Employ multiple levels of verification, not blindly trusting LLM outputs.

**Example High-Risk Scenarios:**

1. News org relies on LLM, spreading misinformation.

2. LLM plagiarizes, hurting trust.

3. Over-reliance on LLM coding suggestions introduces vulnerabilities. 

4. Developer adds malicious package, not verifying LLM suggestion.



### MITRE ATLAS™ 

#### Techniques

N.A. Since Overreliance is not truly an attack vector.

#### Mitigations

- [AML.M0015: Adversarial Input Detection](/mitigations/AML.M0015) - Detect and block inputs that deviate from known benign behavior through techniques like anomaly detection on queries and user input. This can prevent malicious or erroneous inputs from reaching downstream systems and users.

- [AML.M0018: User Training](/mitigations/AML.M0018) - Train users on adversarial machine learning risks so they understand the potential for issues with LLMs and do not blindly trust outputs. Set clear expectations on when manual review is required.

#### Possible Additions

**Possible Additional Mitigations** 

- Modularize Complex Tasks - Break down complex prompts requiring reasoning into smaller subtasks across multiple agents to reduce reliance on any single system. This limits the blast radius from any single erroneous output.

- Responsible Interface Design - Design interfaces to promote safe and responsible use through visibility into model limitations, controls like confidence thresholds, and appropriate framing of LLM capabilities. This prevents users from blindly trusting outputs.

- Oversight for Risky Actions - Require human approval before allowing high-risk actions suggested by LLMs like publishing content or executing code. This acts as a check against blindly trusting potentially unsafe actions.

- Monitor and Log Interactions - Continuously monitor and log user interactions and queries to the LLM. Logs can be analyzed to identify potentially malicious or erroneous inputs as well as find patterns of overreliance for additional user training.

- Independent Oversight - Establish independent oversight teams responsible for auditing logs, reviewing outlier cases, and assessing risks. This provides an unbiased perspective to identify potential issues.


### STRIDE Analysis (generated by claude.ai)

**Spoofing**

- LLMs can convincingly spoof accuracy and authority when generating erroneous or malicious outputs.
- Adversaries can spoof trusted sources and identities when manipulating LLM-generated content.

**Tampering**

- Blind trust enables adversaries to tamper with decision-making, system behaviors, and data flows.
- LLMs may unknowingly propagate altered information when overrelied upon.

**Repudiation** 

- Lack of validation around LLM outputs can complicate attack attribution.
- Overreliance may allow covering up tampering by accepting altered data or behaviors.

**Information Disclosure**

- Unverified LLM outputs could reveal sensitive information to unauthorized parties.
- Confidential data may be accidentally exposed when overrelying on LLM suggestions.

**Elevation of Privilege** 

- Adversaries can potentially escalate privileges by exploiting excessive trust in LLM-guided actions.
- LLM outputs could disable access controls if improperly implemented without oversight.


### Common Weakness Enumeration (CWE)
n.A.



---

# IGNORE FOR NOW - NEED RE-REVIEW


### MITRE ATT&CK® Techniques

N.A.

### MITRE ATT&CK® Mitigations

N.A.



