By Bob Simonoff

- LinkedIn at https://www.linkedin.com/in/bob-simonoff

- medium.com at https://medium.com/@bob.simonoff

- github at https://github.com/Bobsimonoff/LLM-4-Applications-Commentary/tree/main


# LLM09: Overreliance

### Summary

Blindly trusting LLM outputs without verification can lead to harms including security issues, misinformation, legal problems, and reputational damage. Critical assessment of LLM content is essential.

### Description

Overreliance can occur when an Large Language Model produces inaccurate, inappropriate, or unsafe information and provides it in an authoritative manner. When people or systems trust this information without oversight or confirmation it can result in a security breach, misinformation, miscommunication, legal issues, and reputational damage.

The core risk is blind trust in LLM output without verification, which enables harms from erroneous information. Awareness, validation and oversight are key to prevent overreliance.

Not to be confused with:
- LLM02: Insecure Output Handling - Failing to validate LLM-generated outputs before sending them to downstream services can allow insecure code or attack strings to reach vulnerable systems. 


### Common Examples of Risk

1. LLM gives inaccurate information confidently, misleading users. 

2. LLM suggests insecure code, introducing vulnerabilities.


### Prevention and Mitigation Strategies

1. Continuously monitor and manually review LLM outputs.

2. Validate outputs by comparing to trusted external sources.

3. Use automated validations against known facts.

4. Fine-tune models to improve quality. 

5. Break complex tasks into smaller pieces across multiple agents.

6. Clearly communicate risks and limitations of LLMs.

7. Design interfaces that promote safe and responsible LLM use.

8. Establish secure coding practices when using LLM-generated code.

9. Implement human-in-the-loop approval for high risk or sensitive actions. 

10. Employ multiple levels of verification, not blindly trusting LLM outputs.

**Example High-Risk Scenarios:**

1. News org relies on LLM, spreading misinformation.

2. LLM plagiarizes, hurting trust.

3. Over-reliance on LLM coding suggestions introduces vulnerabilities. 

4. Developer adds malicious package, not verifying LLM suggestion.


### Common Weakness Enumeration (CWE)

- [CWE-829](https://cwe.mitre.org/data/definitions/829.html): Inclusion of Functionality from Untrusted Control Sphere

  Description: This weakness relates to the inclusion of functionality from an untrusted control sphere, which can lead to security issues.

  Justification: Inclusion of functionality from untrusted sources, particularly when guided by LLM suggestions, can amplify the risks associated with overreliance, leading to security vulnerabilities or even breaches.

- [CWE-20](https://cwe.mitre.org/data/definitions/20.html): Improper Input Validation
  
  Description: Weakness in input validation can allow an attacker to exploit the system.
  
  Justification: Since LLMs often generate outputs based on inputs, failing to validate those inputs can contribute to overreliance on misleading or harmful outputs.


### Techniques

### MITRE ATT&CK® Techniques

- No changes made due to no suggested additions

### MITRE ATLAS™ Techniques

- [AML.T0045](https://atlas.mitre.org/techniques/AML.T0045/): ML Intellectual Property Theft

  Description: Excessive trust in LLM outputs enables adversaries to more readily manipulate users into improperly disclosing or mishandling intellectual property and other confidential data assets by providing manipulated information that is incorrectly treated as authoritative without proper verification.

  Justification: Overreliance on LLM outputs not only exposes intellectual property to theft but also amplifies the risk of disclosing sensitive or classified information. Users may unknowingly act on manipulated or incorrect LLM suggestions.


### Mitigations

### MITRE ATT&CK® Mitigations

N.A.

### MITRE ATLAS™ Mitigations  

N.A.

### Additional Mitigations

- Model Monitoring

  Description: Continuously monitor model outputs for anomalies or suspicious activities to alert users or administrators.

  Justification: Real-time monitoring can serve as an early warning system, thereby reducing the risk associated with overreliance on LLM outputs.

### STRIDE Analysis (generated by clause.ai)

Overreliance attacks primarily exploit the following components of the STRIDE threat model:

**Spoofing**

- LLMs can convincingly spoof accuracy and authority when generating erroneous or malicious outputs.
- Adversaries can spoof trusted sources and identities when manipulating LLM-generated content.

**Tampering**

- Blind trust enables adversaries to tamper with decision-making, system behaviors, and data flows.
- LLMs may unknowingly alter information when overrelied upon.

**Repudiation** 

- Lack of validation around LLM outputs can complicate attack attribution.
- Overreliance may allow covering up tampering by accepting altered data or behaviors.

**Information Disclosure**

- Unverified LLM outputs could reveal sensitive information to unauthorized parties.
- Confidential data may be accidentally exposed when overrelying on LLM suggestions.

**Elevation of Privilege** 

- Adversaries can potentially escalate privileges by exploiting excessive trust in LLM-guided actions.
- LLM outputs could disable access controls if improperly implemented without oversight.

In summary, overreliance undermines security controls by enabling adversaries to manipulate user perceptions and system behaviors through unvalidated LLM outputs. The impacts span multiple components of STRIDE, with spoofing, tampering, and information disclosure being particularly prevalent risks.
