By Bob Simonoff

- LinkedIn at https://www.linkedin.com/in/bob-simonoff

- medium.com at https://medium.com/@bob.simonoff

- github at https://github.com/Bobsimonoff/LLM-4-Applications-Commentary/tree/main


# LLM09: Overreliance

### Summary

Blindly trusting LLM outputs can lead to issues like misinformation, legal problems, and reputational damage without verification.

### Description

Overreliance can occur when an Large Language Model produces inaccurate, inappropriate, or unsafe information and provides it in an authoritative manner. When people or systems trust this information without oversight or confirmation it can result in a security breach, misinformation, miscommunication, legal issues, and reputational damage.

The core risk is blind trust in LLM output without verification, which enables harms from erroneous information. Awareness, validation and oversight are key to prevent overreliance.

Not to be confused with:
- LLM02: Insecure Output Handling - Failing to validate LLM generated outputs can create an attack vector downstream systems, whereas overreliance is about blindly trusting even validated outputs.


### Common Examples of Risk

1. LLM gives inaccurate information confidently, misleading users. 

2. LLM suggests insecure code, introducing vulnerabilities.


### Prevention and Mitigation Strategies

1. Continuously monitor and manually review LLM outputs.

2. Validate outputs by comparing to trusted external sources.

3. Use automated validations against known facts.

4. Fine-tune models to improve quality. 

5. Break complex tasks into smaller pieces across multiple agents.

6. Clearly communicate risks and limitations of LLMs.

7. Design interfaces that promote safe and responsible LLM use.

8. Establish secure coding practices when using LLM-generated code.

9. Implement human-in-the-loop approval for high risk or sensitive actions. 

10. Employ multiple levels of verification, not blindly trusting LLM outputs.

**Example High-Risk Scenarios:**

1. News org relies on LLM, spreading misinformation.

2. LLM plagiarizes, hurting trust.

3. Over-reliance on LLM coding suggestions introduces vulnerabilities. 

4. Developer adds malicious package, not verifying LLM suggestion.



### MITRE ATLAS™ 

#### Techniques

N.A. Since Overreliance is not truly an attack vector.

#### Mitigations

- [AML.M0015](https://atlas.mitre.org/mitigations/AML.M0015/): Adversarial Input Detection. Detect and block adversarial inputs or atypical queries that deviate from known benign behavior, exhibit behavior patterns observed in previous attacks or that come from potentially malicious IPs. Incorporate adversarial detection algorithms into the ML system prior to the ML model. Prevent an attacker from introducing adversarial data into the system. Monitor queries and query patterns to the target model, block access if suspicious queries are detected. Assess queries before inference call or enforce timeout policy for queries which consume excessive resources. Incorporate adversarial input detection into the pipeline before inputs reach the model.

- [AML.M0018](https://atlas.mitre.org/mitigations/AML.M0018/): User Training. Educate ML model developers on secure coding practices and ML vulnerabilities. Training users to be able to identify attempts at manipulation will make them less susceptible to performing techniques that cause the execution of malicious code. Train users to identify attempts of manipulation to prevent them from running unsafe code which when executed could develop unsafe artifacts. These artifacts may have a detrimental effect on the system.

#### Possible Additions

**Possible Additional Mitigations** 

- AML.MXXXX: Responsible Interface Design - Design interfaces to promote safe and responsible use through visibility into model limitations, controls like confidence thresholds, and appropriate framing of LLM capabilities. This prevents users from blindly trusting outputs.

- AML.MXXXX: Oversight for Risky Actions - Require human approval before allowing high-risk actions suggested by LLMs like publishing content or executing code. This acts as a check against blindly trusting potentially unsafe actions.

- AML.MXXXX: Monitor and Log Interactions - Continuously monitor and log user interactions and queries to the LLM. Logs can be analyzed to identify potentially malicious or erroneous inputs as well as find patterns of overreliance for additional user training.

- AML.MXXXX: Independent Oversight - Establish independent oversight teams responsible for auditing logs, reviewing outlier cases, and assessing risks. This provides an unbiased perspective to identify potential issues.
- 

### STRIDE Analysis (generated by claude.ai)

**Spoofing**

- LLMs can convincingly spoof accuracy and authority when generating erroneous or malicious outputs.
- Adversaries can spoof trusted sources and identities when manipulating LLM-generated content.

**Tampering**

- Blind trust enables adversaries to tamper with decision-making, system behaviors, and data flows.
- LLMs may unknowingly propagate altered information when overrelied upon.

**Repudiation** 

- Lack of validation around LLM outputs can complicate attack attribution.
- Overreliance may allow covering up tampering by accepting altered data or behaviors.

**Information Disclosure**

- Unverified LLM outputs could reveal sensitive information to unauthorized parties.
- Confidential data may be accidentally exposed when overrelying on LLM suggestions.

**Elevation of Privilege** 

- Adversaries can potentially escalate privileges by exploiting excessive trust in LLM-guided actions.
- LLM outputs could disable access controls if improperly implemented without oversight.


### Common Weakness Enumeration (CWE)
n.A.



---

# IGNORE FOR NOW - NEED RE-REVIEW


### MITRE ATT&CK® Techniques

N.A.

### MITRE ATT&CK® Mitigations

N.A.



