By Bob Simonoff

- LinkedIn at https://www.linkedin.com/in/bob-simonoff

- medium/com at https://medium.com/@bob.simonoff

- github at https://github.com/Bobsimonoff/LLM-4-Applications-Commentary/tree/main


# LLM09: Overreliance

### Summary

Failing to critically assess LLM outputs can lead to compromised decision making, security vulnerabilities, and legal liabilities.

### Description

Overreliance can occur when an Large Language Model produces erroneous information and provides it in an authoritative manner. While LLMs can produce creative and informative content, they can also generate content that is factually incorrect, inappropriate or unsafe. This is referred to as hallucination or confabulation. When people or systems trust this information without oversight or confirmation it can result in a security breach, misinformation, miscommunication, legal issues, and reputational damage.


### Common Examples of Risk

1. LLM gives inaccurate information confidently, misleading users. 

2. LLM suggests insecure code, introducing vulnerabilities.


### Prevention and Mitigation Strategies

1. Continuously monitor and manually review LLM outputs.

2. Validate outputs by comparing to trusted external sources.

3. Use automated validations against known facts.

4. Fine-tune models to improve quality. 

5. Break complex tasks into smaller pieces across multiple agents.

6. Clearly communicate risks and limitations of LLMs.

7. Design interfaces that promote safe and responsible LLM use.

8. Establish secure coding practices when using LLM-generated code.

9. Implement human-in-the-loop approval for high risk or sensitive actions. 

10. Employ multiple levels of verification, not blindly trusting LLM outputs.

**Example High-Risk Scenarios:**

1. News org relies on LLM, spreading misinformation.

2. LLM plagiarizes, hurting trust.

3. Over-reliance on LLM coding suggestions introduces vulnerabilities. 

4. Developer adds malicious package, not verifying LLM suggestion.


### Common Weakness Enumeration (CWE)


- [CWE-829](https://cwe.mitre.org/data/definitions/829.html): Inclusion of Functionality from Untrusted Control Sphere

  Description: This weakness relates to the inclusion of functionality from an untrusted control sphere, which can lead to security issues.

  Justification: CWE-829 is applicable because relying on LLM outputs without validating the functionality can risk including untrusted code or logic, potentially leading to security vulnerabilities.


### MITRE ATT&CK® Techniques

N.A.


### MITRE ATLAS™ Techniques

- AML.T0011: User Execution  

  Description: Adversaries may manipulate users into executing unsafe actions or accessing malicious content based on incorrect information generated by an LLM that is over-relied upon. If users trust LLM outputs without proper validation, they may unknowingly execute unsafe code or actions that have been socially engineered by the adversary to take advantage of that blind trust.

  Justification: User execution of unsafe actions guided by over-reliance on incorrect LLM outputs enables adversaries to more readily manipulate user behavior through social engineering tactics designed to exploit the excessive trust placed in the LLM.

- AML.T0045: ML Intellectual Property Theft

  Description: Excessive trust in LLM outputs enables adversaries to more readily manipulate users into improperly disclosing or mishandling intellectual property and other confidential data assets by providing manipulated information that is incorrectly treated as authoritative without proper verification.

  Justification: Over-reliance on LLM outputs allows adversaries to more easily exploit users' excessive trust to steal IP and data through user actions guided by misleading LLM information.

- AML.T0046: Spamming ML System with Chaff Data

  Description: By overwhelming an LLM with excessive useless input designed to increase inaccurate outputs, an adversary can degrade the model's reliability over time, increasing the occurrence of incorrect outputs that users may over-rely on without proper verification.

  Justification: Spamming the LLM with useless chaff inputs manipulates the model to produce more inaccurate outputs, which users may blindly trust due to over-reliance, enabling adversary exploitation through the increased unreliable outputs.


### MITRE ATT&CK® Mitigations

- [M1017](https://attack.mitre.org/mitigations/M1017): User Training

  Description: Train users to be aware of access or manipulation attempts by an adversary to reduce the risk of successful spearphishing, social engineering, and other techniques that involve user interaction.

  Justification: User training helps prevent blind trust and overreliance on LLM outputs by promoting critical assessment.

- [M1042](https://attack.mitre.org/mitigations/M1042): Disable or Remove Feature or Program

  Description: Remove or deny access to unnecessary and potentially vulnerable software to prevent abuse by adversaries.

  Justification: Limiting available services reduces potential vectors for exploiting overreliance.

- [M1030](https://attack.mitre.org/mitigations/M1030): Network Segmentation

  Description: Architect sections of the network to isolate critical systems, functions, or resources. Use physical and logical segmentation to prevent access to potentially sensitive systems and information.

  Justification: Network segmentation can limit impacts of actions guided by overreliance.

- [M1032](https://attack.mitre.org/mitigations/M1032): Multi-factor Authentication

  Description: Use two or more pieces of evidence to authenticate to a system; such as username and password in addition to a token from a physical smart card or token generator.

  Justification: Raises the bar for gaining access needed to manipulate overreliance.


### MITRE ATLAS™ Mitigations

- AML.M0018: User Training
  Description: Educate ML model developers on secure coding practices and ML vulnerabilities.

---

# REVIEW COMMENTS

## Common Weakness Enumeration (CWE)

### Remove

- None: All current CWEs are strongly associated with the risk of overreliance on LLMs.

### Add

- [CWE-20](https://cwe.mitre.org/data/definitions/20.html): Improper Input Validation
  
  Description: Weakness in input validation can allow an attacker to exploit the system.
  
  Justification: Since LLMs often generate outputs based on inputs, failing to validate those inputs can contribute to overreliance on misleading or harmful outputs. 

### Update

- [CWE-829](https://cwe.mitre.org/data/definitions/829.html): Inclusion of Functionality from Untrusted Control Sphere

  Old Justification: This weakness relates to the inclusion of functionality from an untrusted control sphere, which can lead to security issues.

  New Justification: Inclusion of functionality from untrusted sources, particularly when guided by LLM suggestions, can amplify the risks associated with overreliance, leading to security vulnerabilities or even breaches.

## MITRE ATT&CK® Techniques

### Remove

- N.A.

### Add

- N.A.

  Justification: Currently, there are no MITRE ATT&CK® Techniques listed. Inclusion of relevant techniques targeting overreliance could strengthen this section.

## MITRE ATLAS™ Techniques

### Remove

- None: All current techniques are strongly associated with this risk.

### Add

- AML.T0071: Excessive Trust in Model Outputs
  
  Description: Users may trust LLM outputs for decision-making in security-critical scenarios without proper validation.
  
  Justification: Overreliance on LLM outputs can lead to compromised decision-making and security vulnerabilities, making it essential to address this in the ATLAS techniques.

### Update

- AML.T0045: ML Intellectual Property Theft

  Old Justification: Over-reliance on LLM outputs allows adversaries to more easily exploit users' excessive trust to steal IP and data through user actions guided by misleading LLM information.

  New Justification: Overreliance on LLM outputs not only exposes intellectual property to theft but also amplifies the risk of disclosing sensitive or classified information. Users may unknowingly act on manipulated or incorrect LLM suggestions.

## MITRE ATT&CK® Mitigations

### Remove

- None: All current mitigations are relevant to overreliance on LLMs.

### Add

- [M1051](https://attack.mitre.org/mitigations/M1051): Regular Expression Limitations
  
  Description: Use regular expressions to limit and validate inputs and outputs to and from LLMs.
  
  Justification: Implementing regular expression checks can mitigate the risks associated with accepting or generating unsafe or incorrect data, thus countering overreliance on LLMs.

## MITRE ATLAS™ Mitigations

### Remove

- None: All current mitigations are relevant to overreliance on LLMs.

### Add

- AML.M0021: Model Monitoring

  Description: Continuously monitor model outputs for anomalies or suspicious activities to alert users or administrators.

  Justification: Real-time monitoring can serve as an early warning system, thereby reducing the risk associated with overreliance on LLM outputs.

By reviewing each section meticulously, it's clear that most of the existing entries are pertinent and strongly associated with the risk of overreliance on Large Language Models. However, there are areas for improvement, notably in adding more granular CWEs and ATLAS Techniques and Mitigations that address specific vulnerabilities associated with overreliance.

  
---
# Consolidated
---

### Common Weakness Enumeration (CWE)

- [CWE-829](https://cwe.mitre.org/data/definitions/829.html): Inclusion of Functionality from Untrusted Control Sphere

  Description: This weakness relates to the inclusion of functionality from an untrusted control sphere, which can lead to security issues.
  
  Justification: Inclusion of functionality from untrusted sources, particularly when guided by LLM suggestions, can amplify the risks associated with overreliance, leading to security vulnerabilities or even breaches.

- [CWE-20](https://cwe.mitre.org/data/definitions/20.html): Improper Input Validation

  Description: Weakness in input validation can allow an attacker to exploit the system.
  
  Justification: Since LLMs often generate outputs based on inputs, failing to validate those inputs can contribute to overreliance on misleading or harmful outputs.

#### Rejected CWE Review Comments

- None


### Techniques

#### MITRE ATT&CK® Techniques

- No changes made due to no suggested additions

#### MITRE ATLASTM Techniques

- AML.T0045: ML Intellectual Property Theft

  Description: Excessive trust in LLM outputs enables adversaries to more readily manipulate users into improperly disclosing or mishandling intellectual property and other confidential data assets by providing manipulated information that is incorrectly treated as authoritative without proper verification.

  Justification: Overreliance on LLM outputs not only exposes intellectual property to theft but also amplifies the risk of disclosing sensitive or classified information. Users may unknowingly act on manipulated or incorrect LLM suggestions.
  
- AML.T0071: Excessive Trust in Model Outputs

  Description: Users may trust LLM outputs for decision-making in security-critical scenarios without proper validation.

  Justification: Overreliance on LLM outputs can lead to compromised decision-making and security vulnerabilities, making it essential to address this in the ATLAS techniques.

#### Rejected Technique Review Comments

- None

### Mitigations

#### MITRE ATT&CK® Mitigations

- [M1051](https://attack.mitre.org/mitigations/M1051): Regular Expression Limitations

  Description: Use regular expressions to limit and validate inputs and outputs to and from LLMs.

  Justification: Implementing regular expression checks can mitigate the risks associated with accepting or generating unsafe or incorrect data, thus countering overreliance on LLMs. 

#### MITRE ATLASTM Mitigations

- AML.M0021: Model Monitoring

  Description: Continuously monitor model outputs for anomalies or suspicious activities to alert users or administrators.

  Justification: Real-time monitoring can serve as an early warning system, thereby reducing the risk associated with overreliance on LLM outputs.

#### Rejected Mitigation Review Comments

- None


